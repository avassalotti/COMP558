\documentclass{beamer}
\usepackage{amsmath, amsfonts, graphicx}

\title{Gaussian Processes for Signal Strength-Based Location Estimation}
\subtitle{Brian Ferris \quad Dirk Hahnel \quad Dieter Fox}

\author{Alexandre Vassalotti}
\begin{document}

\begin{frame}[plain]
  \titlepage
\end{frame}

\begin{frame}{Introduction}
  \begin{itemize}
  \item Method for estimating postion using WiFi and GSM cellphone signal
    strength.
  \item Difficult problem because we can't predict \emph{reliably} how signals
    will propagate in an indoor environment.
  % orientation matters
  % cant assume white noise 
  \item And important because WiFi and cellphone signal are everywhere.
  \item Not the same GPS!
  \end{itemize}
\end{frame}

\begin{frame}{Two classes of techniques}
  \begin{itemize}
  \item \textbf{Modeling}
  \item \textbf{Mapping}
  \end{itemize}
\end{frame}

\begin{frame}{Signal strength modeling}
  \begin{enumerate}
  \item Assume we have \emph{extensive} knowledge of the environment (e.g., we
    know where are the signal sources, what the walls are made of, etc)
  \item Then, work through some signal propagation model to find the expected
    signal strength at a given location.
  \end{enumerate}
\end{frame}

\begin{frame}{Issues with modeling}
  \begin{itemize}
  \item \textbf{Impractical assumptions}, these are inaccurate even if we
    start taking account to location of furnitures.
  \item \textbf{Wrong approach}, it tries to solve the localization problem
    from its hardest angle.
  \end{itemize}
\end{frame}

\begin{frame}{Key observation}
  Measurements are easier (and cheaper!) than simulation.
\end{frame}

\begin{frame}{Signal strength mapping}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{enumerate}
      \item Collect signal information at many different locations.
      \item Extract relevant statistics from this calibration data.
      \item And use this to compute measurement likelihoods for locations.
      \end{enumerate}
    \end{column}
    \pause
    \begin{column}{0.5\textwidth}
      \begin{itemize}
      \item But how much is enough to achieve accuracy?
        \pause
      \item Difficult to consider all the available information.
        \pause
      \item How we should generate likelihood where we don't have 
        calibration data?
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Formally}
  Given a set of observations $\mathcal{D} = \{(\mathbf{x}_1,y_1),
  (\mathbf{x}_2,y_2), \dots, (\mathbf{x}_n,y_n)\}$ where
  $\mathbf{x}_i\in\mathbb{R}^d$ and $y_i\in\mathbb{R}$, we want to find
  $f$ which predicts all possible values.
  \pause
  \begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{observ.pdf}
  \end{figure}
\end{frame}

\begin{frame}{Our goal}
  \emph{In other words, we have a supervised learning problem.}
\end{frame}

\begin{frame}{Two common approaches}
  We need to make some assumptions about the function $f$.
  \begin{itemize}
  \item We can restrict $f$ to a class of functions (e.g., linear, quadratic,
    exponential, etc).
  \item Or, we can give a \emph{prior} probability to every possible functions
    (e.g., we can consider smooth functions as more likely).
  \end{itemize}
\end{frame}

\begin{frame}{Gaussian processes}
  \begin{center}
    \Large Demo
  \end{center}
\end{frame}

\begin{frame}{Gaussian processes}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{gaussdemo.pdf}
  \end{figure}
\end{frame}

\begin{frame}{Gaussian processes}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{gaussdemo2d.pdf}
  \end{figure}
\end{frame}

\begin{frame}{Gaussian processes}
  \begin{definition}
    \textit{A Gaussian process is a collection of random variables, where any
      finite number of which have a joint Gaussian distribution.}
  \end{definition}
  Completely defined by its mean function and covariance function, or kernel
  $k(\mathbf{x}, \mathbf{x}')$.
\end{frame}

\begin{frame}{Gaussian processes regression with noisy observations}
  Let
  $\mathcal{D}=\{(\mathbf{x}_1,y_1),(\mathbf{x}_2,y_2),\dots,(\mathbf{x}_n,y_n)\}$
  be a training sample, then
  \[
    y_i = f(\mathbf{x}_i) + \epsilon
  \]
  where $\mathbf{x}_i$ is an input sample in $\mathbb{R}^d$, $y_i$ is an
  observation and $\epsilon \sim \mathcal{N}(0, \sigma_n^2)$.
\end{frame}

\begin{frame}{Covariance of noisy observations}
  This is our prior over functions.
  \[
  \mathrm{cov}(y_p, y_q) = k(\mathb{x}_p, \mathb{x}_q) + \sigma_n^2 \delta_{pq}
  \]
  or
  \[
  \mathrm{cov}(\mathbf{y}) = K + \sigma^2_nI
  \]
  where $\delta_{pq}$ is the Kronecker delta.
\end{frame}

\begin{frame}{Kernel function}
  \[
     k(\mathb{x}_p, \mathb{x}_q) =
     \sigma_f^2 \exp\left(\frac{1}{2\ell^2}|\mathb{x}_p - \mathb{x}_q|^2\right)
  \]
  where $\sigma_f^2$ is the signal variance and $\ell$ the length scale.
\end{frame}

\begin{frame}{Posterior distribution over functions}
  This is the core of our regression. Here we predict the function at some
  point $\mathbf{x}_*$, conditioned on the training data $\mathbf{X},\mathbf{y}$
  \[
  p(f(\mathbf{x})|\mathbf{x}_{*},\mathbf{X},\mathbf{y}) 
  \sim \mathcal{N}(f(\mathbf{x}); \mu_x, \sigma_{x}^2) 
  \]
  \begin{align*}
  \mu_\mathbf{x_*} &= \mathbf{k}_{*}^{T} (K  + \sigma_n^2I)^{-1} \mathbf{y} \\
  \sigma^2_{\mathbf{x}_*} &= k(\mathbf{x}_{*}, \mathbf{x}_{*}) - 
  \mathb{k}_{*}^{T}(K + \sigma^2_nI)^{-1}\mathbf{k}_{*}
  \end{align*}
  Just yank predicted value at $\mathbf{x_*}$ with the MLE.
\end{frame}

\begin{frame}{Demo}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{gpr.pdf}
  \end{figure}
\end{frame}

\begin{frame}{Hyperparameter estimation}
  Where $\sigma^2_n$, $\ell$ and $\sigma^2_f$ in our prior come from?
  \begin{itemize}
  \item Estimate them from calibration data.
  \item Optimize using gradient descent over the log-likelihood.
  \item Expensive step. Running time is $O(n^3)$.
  \end{itemize}
  How do we deal with local minima?
\end{frame}

\begin{frame}{Zero mean offset}
  \begin{itemize}
  \item Gaussian process is by default zero mean.
  \item Signal modeling need a more nuanced approach.
    \pause
  \item We want points far from the AP to tend toward zero. Yet, we want
    regions near the AP without training data to not tend to zero completely.
  \end{itemize}
  \pause
  Simple trick: add a linear offset proportional to the distance from the
  AP. Assume we know where the AP is?
\end{frame}

\begin{frame}{Recap}
  The good things:
  \begin{itemize}
  \item \textbf{Correct uncertainty handling}, GPs provides uncertainty
    estimate which accounts for data density and noise.
  \item \textbf{Arbitrary likelihood models}, GPs are able to approximate
    a wide range of non-linear models.
  \item \textbf{Consistent parameter estimation}, parameters of GP are learned
    from calibration data.
  \end{itemize}
\end{frame}

\begin{frame}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{measure.png}
  \end{figure}
\end{frame}

\begin{frame}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{var.png}
  \end{figure}
\end{frame}

\begin{frame}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{mean.png}
  \end{figure}
\end{frame}


\begin{frame}{Localization}
  \centering
  \textit{``We have a map, getting lost is not option!''}
\end{frame}

\begin{frame}{Bayesian localization}
\begin{block}{Goal}
Estimate the posterior over a person's location, $x_t$,
conditioned on all sensor measurements $z_{1:t}$ obtained through
\mbox{time $t$}.
\end{block}
\pause
\emph{This should should awfully familar...}
\end{frame}

\begin{frame}{Bayesian Filtering}
  \begin{itemize}
  \item Like Kalman Filtering, but without the assumption of linearity
    or of normality on the variables.
  \item Observation model is the Gaussian process.
  \end{itemize}
\end{frame}

\begin{frame}{Bayes filter}
  Recursive predict step
  \[
  p(x_t|z_{1:t}) \propto p(z_t|x_t) \int p(x_t|x_{t-1}) p(x_{t-1}|z_{1:t-1})\, dx_{1:t-1}
  \]
  where $x_t$ is the location at time $t$ and $z_{1:t}$ is all sensor
  measurements through time $t$. $p(z_t|x_t)$ is extracted from the
  GP. $p(x_t|x_{t-1})$ is the motion model.
\end{frame}

\begin{frame}{Bayesian filtering}
  \begin{itemize}
  \item They use a mixed graph and free space representation to constraint
    motion.
  \item Smoothing is required to avoid overconfident estimates if we take many
    readings at a location (e.g., multiple APs).
  \item Adding an explicit probability for not detecting a source helps.
  \item Bayes filter implemented using particle filters.
  \end{itemize}
\end{frame}

\begin{frame}{Mixed graph and free space representation}
  \begin{figure}
    \centering
    \includegraphics[width=.6\textwidth]{map.png}
  \end{figure}
\end{frame}

\begin{frame}{Experimental Results}
  \begin{itemize}
  \item Average error over a 3 km test run was 2.12 meters.
  \item Correct room prediction 80\% of the time.
  \item Graceful degradation under sparse training data.
  \item Application to GSM signals.
  \end{itemize}
\end{frame}

\begin{frame}{Final thoughts}
  \begin{itemize}
  \item They assume independence of the signal source.
  \item No justifications for the use of the squared exponential kernel.
  \item Assume orientation doesn't matter.
  \item Motion model is a heuristic.
  \item Expensive computations for the hyper-parameters estimation and the
    inversion of the $(K+\sigma_n^2I)^{-1}$ term.
  \end{itemize}
\end{frame}

\begin{frame}{Further readings}
  \begin{figure}
    \centering
    \includegraphics[width=.3\textwidth]{book.jpg}
  \end{figure}
  C.E. Rasmussen and C.K.I. Williams. \textit{Gaussian processes
    for machine learning.} The MIT Press, 2005.
\end{frame}

\begin{frame}{Questions?}
  
\end{frame}

\end{document}
